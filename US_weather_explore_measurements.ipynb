{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read(\"capstone.cfg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(config[\"PATH\"][\"project\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create spark session. Add driver postgress to enable to load from existing postgres DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add driver postgress to enable to load from existing postgres DB\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"US_weather\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.2.18.jar\")\\\n",
    "    .config( \"spark.driver.extraClassPath\", \"postgresql-42.2.18.jar\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to postgres; read stations table from postgres DB \"covid\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weather stations location data from postgres DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = spark.read.format(\"jdbc\")\\\n",
    "    .option(\"url\" , \"jdbc:postgresql://localhost:5432/covid\")\\\n",
    "    .option(\"dbtable\", \"stations\")\\\n",
    "    .option(\"user\",\"sb\")\\\n",
    "    .option(\"password\", \"sb\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load world US weather (prefiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from raw 2020.csv, filter to keep only US stations, and remove failed measurements.\n",
    "See US_weather_exporation.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "weather_path = os.path.join(config[\"PATH\"][\"project\"], \"weather_2020_with_stations.parquet/\" )\n",
    "weather_2020 = spark.read.load(weather_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2020.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "nb_weather_records = weather_2020.count()\n",
    "nb_weather_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elements = weather_2020.select(\"measured\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_measurements= [\"SNOW\", \"SNWD\", \"PRCP\", \"TMAX\", \"TMIN\", \"TAVG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndays_station_msr = weather_2020.groupBy(\"station_id\", \"measured\").agg( F.countDistinct(\"date\").alias(\"nb_days\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep only measurements for main elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndays_station_msr_filterered = ndays_station_msr.filter( ndays_station_msr[\"measured\"].isin(l_measurements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_msr_filtered2.selectdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ndays_station_msr_filterered.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep only (station, element) were nb of measures > 350 over the year (i.e. almost 1 measure per day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "stations_msr_filtered2 = ndays_station_msr_filterered.filter( ndays_station_msr[\"nb_days\"] > 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_filtered_stations = stations_msr_filtered2.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb of stations with at least 1 main element measured most of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "len(df_filtered_stations[\"station_id\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nb of stations per measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Pyspark**\n",
    "\n",
    "`\n",
    "Row(measured='TMIN', count(DISTINCT station_id)=5668),\n",
    " Row(measured='TMAX', count(DISTINCT station_id)=5713),\n",
    " Row(measured='SNOW', count(DISTINCT station_id)=3505),\n",
    " Row(measured='SNWD', count(DISTINCT station_id)=3931),\n",
    " Row(measured='TAVG', count(DISTINCT station_id)=2222),\n",
    " Row(measured='PRCP', count(DISTINCT station_id)=11699)]\n",
    "stations_msr_filtered2\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df_filtered_stations[[\"station_id\", \"measured\"]].groupby(\"measured\").nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogramm of nb stations for number of elements measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** From Pyspark **\n",
    "\n",
    "` [Row(n_elem_measured=6, count=181),\n",
    " Row(n_elem_measured=5, count=2563),\n",
    " Row(n_elem_measured=1, count=6265),\n",
    " Row(n_elem_measured=3, count=3250),\n",
    " Row(n_elem_measured=2, count=707),\n",
    " Row(n_elem_measured=4, count=352)]\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_filtered_stations[\"exist\"] = 1\n",
    "df_pivot_measure = df_filtered_stations.pivot(index = \"station_id\", columns= \"measured\", values = \"exist\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_pivot_measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure[\"flag\"] = 1\n",
    "df_pivot_measure.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure_2 = df_pivot_measure.fillna(0)\n",
    "for measure in l_measurements:\n",
    "    df_pivot_measure_2[measure] = df_pivot_measure_2[measure].apply( lambda x : int(x))\n",
    "df_pivot_measure_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PB WITH GROUP BY !!!!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure_2.reset_index().groupby([\"SNOW\", \"SNWD\", \"PRCP\"]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indicator_fun(l_cols) :\n",
    "    res = 0\n",
    "    for i, col in enumerate(l_cols) :\n",
    "        res = res + (1<<i) * col\n",
    "    return res\n",
    "    \n",
    "df_pivot_measure_2[\"indicator\"] = df_pivot_measure_2[\"PRCP\"] \\\n",
    "                                + 2 * df_pivot_measure_2[\"SNOW\"]\\\n",
    "                                + 4 * df_pivot_measure_2[\"SNWD\"]\\\n",
    "                                + 8 * df_pivot_measure_2[\"TAVG\"]\\\n",
    "                                + 16 * df_pivot_measure_2[\"TMAX\"]\\\n",
    "                                + 32 * df_pivot_measure_2[\"TMIN\"]     \n",
    "df_pivot_measure_2[\"indicator\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure_2[\"indicator\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_measure_2.groupby(\"indicator\").agg( { \"SNOW\" : max,\n",
    "                                              \"SNWD\" : max,\n",
    "                                              \"PRCP\" : max,\n",
    "                                              \"TMAX\" : max,\n",
    "                                              \"TMIN\" : max,\n",
    "                                              \"TAVG\" : max,\n",
    "                                              \"flag\" : sum }).sort_values(\"flag\", ascending= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
